import os
import torch
import pickle
import json
from torch.utils.data import TensorDataset, DataLoader


def load_incremental_data(update_stats=True):
    """åŠ è½½å¢é‡å­¦ä¹ æ•°æ®ï¼ˆå°æ ·æœ¬ï¼‰ï¼Œè¯»å–æ—§å‚æ•°å¹¶åœ¨è®­ç»ƒåæ›´æ–°"""
    import os
    print("åŠ è½½å¢é‡æ•°æ®...")
    new_x = pickle.load(open("./Configuration/dataX.pkl", "rb"))
    new_y = pickle.load(open("./Configuration/dataY.pkl", "rb"))

    new_x = torch.FloatTensor(new_x)
    new_y = torch.FloatTensor(new_y)

    # ------------------------------
    # 1ï¸âƒ£ åŠ è½½æ—§çš„æ ‡å‡†åŒ–å‚æ•°
    # ------------------------------
    if os.path.exists("./Configuration/x_mean.pt"):
        x_mean_old = torch.load("./Configuration/x_mean.pt")
        x_std_old = torch.load("./Configuration/x_std.pt")
        y_mean_old = torch.load("./Configuration/y_mean.pt")
        y_std_old = torch.load("./Configuration/y_std.pt")
        print("âœ… æˆåŠŸåŠ è½½æ—§çš„æ ‡å‡†åŒ–å‚æ•°")
    else:
        # å¦‚æœç¬¬ä¸€æ¬¡è®­ç»ƒï¼ˆæ²¡æœ‰æ—§å‚æ•°ï¼‰
        x_mean_old = new_x.mean(dim=0, keepdim=True)
        x_std_old = new_x.std(dim=0, keepdim=True) + 1e-8
        y_mean_old = new_y.mean(dim=0, keepdim=True)
        y_std_old = new_y.std(dim=0, keepdim=True)
        print("âš ï¸ æœªæ£€æµ‹åˆ°æ—§æ ‡å‡†åŒ–å‚æ•°ï¼Œä½¿ç”¨å½“å‰æ•°æ®åˆå§‹åŒ–")

    # ------------------------------
    # 2ï¸âƒ£ ä½¿ç”¨æ—§å‚æ•°æ ‡å‡†åŒ–æ–°æ•°æ®
    # ------------------------------
    new_x_norm = (new_x - x_mean_old) / x_std_old
    new_y_norm = (new_y - y_mean_old) / y_std_old

    print(f"å¢é‡æ•°æ®å½¢çŠ¶: x={new_x_norm.shape}, y={new_y_norm.shape}")

    # ------------------------------
    # 3ï¸âƒ£ è®­ç»ƒç»“æŸåå¯æ›´æ–°æ ‡å‡†åŒ–å‚æ•°
    # ------------------------------
    if update_stats:
        print("ğŸ” æ›´æ–°å…¨å±€æ ‡å‡†åŒ–å‚æ•°ï¼ˆæŒ‡æ•°æ»‘åŠ¨å¹³å‡ï¼‰")

        # è®¡ç®—æ–°æ ·æœ¬ç»Ÿè®¡
        new_x_mean = new_x.mean(dim=0, keepdim=True)
        new_x_std = new_x.std(dim=0, keepdim=True) + 1e-8
        new_y_mean = new_y.mean(dim=0, keepdim=True)
        new_y_std = new_y.std(dim=0, keepdim=True) + 1e-8

        # æŒ‡æ•°æ»‘åŠ¨å¹³å‡ï¼ˆEMAï¼‰æ›´æ–°
        alpha = 0.2  # å¯è°ƒï¼š0.1~0.3 è¾ƒç¨³å¥
        x_mean_updated = (1 - alpha) * x_mean_old + alpha * new_x_mean
        x_std_updated = (1 - alpha) * x_std_old + alpha * new_x_std
        y_mean_updated = (1 - alpha) * y_mean_old + alpha * new_y_mean
        y_std_updated = (1 - alpha) * y_std_old + alpha * new_y_std

        # ä¿å­˜æ–°å‚æ•°
        torch.save(x_mean_updated, "./Configuration/x_mean.pt")
        torch.save(x_std_updated, "./Configuration/x_std.pt")
        torch.save(y_mean_updated, "./Configuration/y_mean.pt")
        torch.save(y_std_updated, "./Configuration/y_std.pt")

        print("ğŸ’¾ æ–°æ ‡å‡†åŒ–å‚æ•°å·²ä¿å­˜ (èåˆæ—§å‚æ•° + æ–°æ•°æ®ç»Ÿè®¡)")

    return TensorDataset(new_x_norm, new_y_norm)


def create_adapter_model_correct():
    """åˆ›å»ºæ­£ç¡®çš„é€‚é…å™¨æ¨¡å‹ - åŸºäºåŸå§‹UNetExç»“æ„"""
    from MY_Models.UNetEx import UNetEx

    # åŠ è½½åŸå§‹æ¨¡å‹é…ç½®
    with open("./Configuration/model_config.json", 'r') as f:
        config = json.load(f)

    print(f"ğŸ¯ åˆ›å»ºåŸºç¡€æ¨¡å‹: {len(config['filters'])}å±‚ç»“æ„")

    # åˆ›å»ºåŸå§‹UNetExæ¨¡å‹
    base_model = UNetEx(
        in_channels=config['in_channels'],
        out_channels=config['out_channels'],
        filters=config['filters'],
        kernel_size=config['kernel_size'],
        layers=3,
        weight_norm=config['weight_norm'],
        batch_norm=config['batch_norm']
    )

    # åŠ è½½é¢„è®­ç»ƒæƒé‡
    base_model.load_state_dict(torch.load("./Configuration/model_complete.pth"))

    # åˆ›å»ºé€‚é…å™¨åŒ…è£…å™¨
    adapter_model = AdapterWrapper(base_model, adapter_reduction=4)

    return adapter_model, config


class AdapterWrapper(torch.nn.Module):
    """é€‚é…å™¨åŒ…è£…å™¨ - åœ¨åŸå§‹UNetExåŸºç¡€ä¸Šæ·»åŠ é€‚é…å±‚"""

    def __init__(self, base_model, adapter_reduction=4):
        super().__init__()
        self.base_model = base_model

        # å†»ç»“åŸºç¡€æ¨¡å‹çš„æ‰€æœ‰å‚æ•°
        for param in self.base_model.parameters():
            param.requires_grad = False

        # ä¸ºæ¯ä¸ªç¼–ç å™¨å±‚æ·»åŠ é€‚é…å™¨
        self.adapters = torch.nn.ModuleList()
        for i, encoder_block in enumerate(self.base_model.encoder):
            # è·å–è¯¥å±‚çš„è¾“å‡ºé€šé“æ•°
            if hasattr(encoder_block[0], 'weight'):
                out_channels = encoder_block[0].weight.shape[0]
            else:
                # ä¼°è®¡é€šé“æ•°
                out_channels = [8, 16, 32, 32, 64, 64, 128][i]

            adapter = torch.nn.Sequential(
                torch.nn.Conv2d(out_channels, out_channels // adapter_reduction, 1),
                torch.nn.ReLU(),
                torch.nn.Conv2d(out_channels // adapter_reduction, out_channels, 1)
            )
            self.adapters.append(adapter)

        print(f"ğŸ”§ æ·»åŠ äº† {len(self.adapters)} ä¸ªé€‚é…å™¨")

    def forward(self, x):
        # ä½¿ç”¨åŸºç¡€æ¨¡å‹çš„ç¼–ç è¿‡ç¨‹ï¼Œä½†é€šè¿‡é€‚é…å™¨
        tensors = []
        indices = []
        sizes = []

        # ç¼–ç è¿‡ç¨‹ï¼ˆé€šè¿‡é€‚é…å™¨ï¼‰
        for i, encoder in enumerate(self.base_model.encoder):
            x = encoder(x)

            # é€šè¿‡é€‚é…å™¨
            if i < len(self.adapters):
                x = x + self.adapters[i](x)  # æ®‹å·®è¿æ¥

            sizes.append(x.size())
            tensors.append(x)
            x, ind = torch.nn.functional.max_pool2d(x, 2, 2, return_indices=True)
            indices.append(ind)

        # ä½¿ç”¨åŸºç¡€æ¨¡å‹çš„è§£ç è¿‡ç¨‹
        x = self.base_model.decode(x, tensors, indices, sizes)

        return x

    def get_adapter_parameters(self):
        """è¿”å›æ‰€æœ‰é€‚é…å™¨å‚æ•°ï¼ˆç”¨äºä¼˜åŒ–ï¼‰"""
        return list(self.adapters.parameters())

    def train_adapters_only(self):
        """ç¡®ä¿åªæœ‰é€‚é…å™¨å‚æ•°å¯è®­ç»ƒ"""
        for param in self.base_model.parameters():
            param.requires_grad = False
        for param in self.adapters.parameters():
            param.requires_grad = True


def incremental_training_correct():
    """æ­£ç¡®çš„å°æ ·æœ¬å¢é‡å­¦ä¹ å®ç°"""
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")

    # åˆ›å»ºæ­£ç¡®çš„é€‚é…å™¨æ¨¡å‹
    model, config = create_adapter_model_correct()
    model.to(device)

    # ç¡®ä¿åªæœ‰é€‚é…å™¨å¯è®­ç»ƒ
    model.train_adapters_only()

    # æ‰“å°å¯è®­ç»ƒå‚æ•°ä¿¡æ¯
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total_params = sum(p.numel() for p in model.parameters())
    print(f"ğŸ“ˆ æ¨¡å‹å‚æ•°: å¯è®­ç»ƒ {trainable_params:,} / æ€»è®¡ {total_params:,}")
    print(f"ğŸ”’ å†»ç»“æ¯”ä¾‹: {(1 - trainable_params / total_params) * 100:.2f}%")

    # åŠ è½½å¢é‡æ•°æ®
    incremental_dataset = load_incremental_data()

    # ä»…ä¼˜åŒ–é€‚é…å™¨å‚æ•°
    adapter_params = model.get_adapter_parameters()
    optimizer = torch.optim.AdamW(adapter_params, lr=5e-5, weight_decay=1e-5)  # åŸæ–‡ä½¿ç”¨5e-5

    # æŸå¤±å‡½æ•°ï¼ˆä¸åŸå§‹è®­ç»ƒä¿æŒä¸€è‡´ï¼‰
    channels_weights = torch.load("./Configuration/channels_weights.pt").to(device)

    def loss_func(output, y):
        losst = ((output[:, 0, :, :] - y[:, 0, :, :]) ** 2).reshape(
            (output.shape[0], 1, output.shape[2], output.shape[3]))
        lossu = ((output[:, 1, :, :] - y[:, 1, :, :]) ** 2).reshape(
            (output.shape[0], 1, output.shape[2], output.shape[3]))
        lossp = torch.abs((output[:, 2, :, :] - y[:, 2, :, :])).reshape(
            (output.shape[0], 1, output.shape[2], output.shape[3]))
        loss = (lossu + losst + lossp) / channels_weights
        return torch.sum(loss)

    # è®­ç»ƒå‰æµ‹è¯•å‰å‘ä¼ æ’­
    print("\nğŸ§ª æµ‹è¯•å‰å‘ä¼ æ’­...")
    try:
        with torch.no_grad():
            test_batch = next(iter(DataLoader(incremental_dataset, batch_size=2)))
            test_x, test_y = test_batch
            test_x, test_y = test_x.to(device), test_y.to(device)
            test_output = model(test_x)
            print(f"âœ… å‰å‘ä¼ æ’­æµ‹è¯•æˆåŠŸ!")
            print(f"   è¾“å…¥: {test_x.shape} -> è¾“å‡º: {test_output.shape}")
    except Exception as e:
        print(f"âŒ å‰å‘ä¼ æ’­æµ‹è¯•å¤±è´¥: {e}")
        return None, []

    # è®­ç»ƒå¾ªç¯ï¼ˆæŒ‰ç…§åŸæ–‡å‚æ•°ï¼‰
    print("\nğŸ¯ å¼€å§‹å°æ ·æœ¬å¢é‡è®­ç»ƒï¼ˆAdapter Tuningï¼‰...")
    print("   é…ç½®: lr=5e-5, batch_size=2, epochs=500")

    model.train()
    train_losses = []
    best_loss = float('inf')
    patience = 50
    patience_counter = 0

    epochs = 1000
    batch_size = 2  # åŸæ–‡ä½¿ç”¨batch_size=2

    for epoch in range(epochs):
        epoch_loss = 0
        batch_count = 0

        for batch in DataLoader(incremental_dataset, batch_size=batch_size, shuffle=True):
            x, y = batch
            x, y = x.to(device), y.to(device)

            optimizer.zero_grad()
            output = model(x)
            loss = loss_func(output, y)
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()
            batch_count += 1

        avg_loss = epoch_loss / len(incremental_dataset)
        train_losses.append(avg_loss)

        # æ—©åœæ£€æŸ¥
        if avg_loss < best_loss:
            best_loss = avg_loss
            patience_counter = 0
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            torch.save(model.state_dict(), "./best_adapter_model.pth")
        else:
            patience_counter += 1

        if (epoch + 1) % 50 == 0 or (epoch + 1) <= 10:
            print(f"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.6f}")

        if patience_counter >= patience:
            print(f"ğŸ›‘ æ—©åœè§¦å‘äºç¬¬ {epoch + 1} è½®")
            break

    # åŠ è½½æœ€ä½³æ¨¡å‹
    model.load_state_dict(torch.load("./best_adapter_model.pth"))
    print(f"\nâœ… å°æ ·æœ¬å¢é‡å­¦ä¹ å®Œæˆï¼æœ€ä½³æŸå¤±: {best_loss:.6f}")

    # è¯„ä¼°
    print("\nğŸ“Š åœ¨æ–°æ„å‹ä¸Šè¯„ä¼°æ¨¡å‹...")
    model.eval()
    with torch.no_grad():
        test_x, test_y = incremental_dataset[:]
        test_x, test_y = test_x.to(device), test_y.to(device)

        predictions = model(test_x)
        error = torch.abs(predictions - test_y)

        mse = torch.mean(error ** 2)
        mae = torch.mean(error)

        # é€šé“çº§åˆ«çš„è¯„ä¼°
        mse_channels = torch.mean(error ** 2, dim=(0, 2, 3))
        mae_channels = torch.mean(error, dim=(0, 2, 3))

        print("è¯„ä¼°ç»“æœ:")
        print(f"  MSE: {mse.item():.6f}")
        print(f"  MAE: {mae.item():.6f}")
        print(f"  é€šé“ MSE: [T: {mse_channels[0]:.6f}, U: {mse_channels[1]:.6f}, P: {mse_channels[2]:.6f}]")
        print(f"  é€šé“ MAE: [T: {mae_channels[0]:.6f}, U: {mae_channels[1]:.6f}, P: {mae_channels[2]:.6f}]")

        # =========================
        # âœ… å¯è§†åŒ–ä¸æ¨¡å‹ä¿å­˜
        # =========================
        print("\nğŸ“ˆ ç»˜åˆ¶è®­ç»ƒæŸå¤±æ›²çº¿...")
        import matplotlib.pyplot as plt
        from functions import visualize  # å¯¼å…¥ä½ çš„å¯è§†åŒ–å‡½æ•°

        # ç»˜åˆ¶æŸå¤±æ›²çº¿
        plt.figure(figsize=(8, 5))
        plt.plot(train_losses, label='Training Loss', color='blue', linewidth=2)
        plt.xlabel('Epoch', fontsize=12)
        plt.ylabel('Loss', fontsize=12)
        plt.title('Incremental Training Loss Curve', fontsize=14)
        plt.legend()
        plt.grid(True)
        plt.tight_layout()
        plt.savefig("./loss_curve.png", dpi=300)
        plt.show()
        print("âœ… è®­ç»ƒæŸå¤±æ›²çº¿å·²ä¿å­˜ä¸º loss_curve.png")

        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        torch.save(model.state_dict(), "./final_adapter_model.pth")
        print("ğŸ’¾ æ¨¡å‹æƒé‡å·²ä¿å­˜åˆ°: ./final_adapter_model.pth")

        # =========================
        # âœ… CFD å¯è§†åŒ–è¾“å‡ºï¼ˆ3 å¼ æ ·ä¾‹ï¼‰
        # =========================
        try:
            print("\nğŸŒˆ ç»˜åˆ¶ 3 å¼ é¢„æµ‹å¯è§†åŒ–æ ·ä¾‹...")

            # åŠ è½½æ ‡å‡†åŒ–å‚æ•°
            y_mean = torch.load("./Configuration/y_mean.pt").to(device)
            y_std = torch.load("./Configuration/y_std.pt").to(device)

            # ä¸ºé˜²æ­¢ GPU å ç”¨è¿‡å¤šï¼Œå…ˆè½¬å› CPU
            sample_y = test_y.cpu()
            out_y = predictions.cpu()
            error_map = error.cpu()

            # ç»˜åˆ¶å‰ä¸‰ä¸ªæ ·æœ¬
            for s in range(3):
                visualize(
                    sample_y=sample_y.unsqueeze(0),
                    out_y=out_y.unsqueeze(0),
                    error=error_map.unsqueeze(0),
                    s=s,
                    y_mean=y_mean,
                    y_std=y_std
                )
                print(f"âœ… å¯è§†åŒ–å®Œæˆ flow_field_{s}.png")

        except Exception as e:
            print(f"âš ï¸ å¯è§†åŒ–ç»˜åˆ¶å¤±è´¥: {e}")

    return model, train_losses


if __name__ == "__main__":
    print("=" * 60)
    print("          å°æ ·æœ¬å¢é‡å­¦ä¹  - æ­£ç¡®å®ç°")
    print("=" * 60)
    print("ğŸ“ ç­–ç•¥: å†»ç»“åŸºç¡€æ¨¡å‹ï¼Œåªè®­ç»ƒé€‚é…å±‚")
    print("ğŸ“ å‚æ•°: lr=5e-5, batch_size=2, æ—©åœæœºåˆ¶")
    print("=" * 60)

    model, losses = incremental_training_correct()
    if model is not None:
        print("\nğŸ‰ Adapter Tuning æˆåŠŸå®Œæˆï¼")
        if losses:
            print(f"   åˆå§‹æŸå¤±: {losses[0]:.6f}")
            print(f"   æœ€ç»ˆæŸå¤±: {losses[-1]:.6f}")
    else:
        print("\nğŸ’¥ å¢é‡å­¦ä¹ å¤±è´¥ï¼")